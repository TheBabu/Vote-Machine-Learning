{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import math\n",
    "from IPython import display\n",
    "from matplotlib import cm\n",
    "from matplotlib import gridspec\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.data import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read CSV File and Randomize**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#How likely to be Republican\n",
    "votes_dataframe = pd.read_csv(\"vote_csv.csv\")\n",
    "votes_dataframe = votes_dataframe.replace(\"y\", 1)\n",
    "votes_dataframe = votes_dataframe.replace(\"n\", 0)\n",
    "votes_dataframe = votes_dataframe.replace(\"republican\", 1)\n",
    "votes_dataframe = votes_dataframe.replace(\"democrat\", 0)\n",
    "\n",
    "#Fill Unfilled Data\n",
    "#votes_dataframe = votes_dataframe.fillna(0.5)\n",
    "\n",
    "#Drop Unfilled Data\n",
    "votes_dataframe.dropna(inplace=True)\n",
    "\n",
    "votes_dataframe = votes_dataframe.reindex(np.random.permutation(votes_dataframe.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create Training and Validation Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_features(votes_dataframe):\n",
    "    output_features = votes_dataframe.iloc[:,0:16]\n",
    "    \n",
    "    return output_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Single Panda Series\n",
    "def preprocess_targets(votes_dataframe):\n",
    "    output_targets = votes_dataframe[\"Class\"]\n",
    "\n",
    "    return output_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training_split = int(0.6 * len(votes_dataframe))\n",
    "validations_split = int(0.3 * len(votes_dataframe))\n",
    "\n",
    "#Panda Data\n",
    "training_examples = preprocess_features(votes_dataframe.iloc[0:training_split])\n",
    "training_targets = preprocess_targets(votes_dataframe.iloc[0:training_split])\n",
    "validation_examples = preprocess_features(votes_dataframe.iloc[training_split:training_split + validations_split])\n",
    "validation_targets = preprocess_targets(votes_dataframe.iloc[training_split:training_split + validations_split])\n",
    "#Do not touch until end!\n",
    "test_examples = preprocess_features(votes_dataframe.iloc[training_split + validations_split:])\n",
    "test_targets = preprocess_targets(votes_dataframe.iloc[training_split + validations_split:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From Google Crash Course\n",
    "#Create Tensorflow Columns\n",
    "def construct_feature_columns(input_features):\n",
    "  \"\"\"Construct the TensorFlow Feature Columns.\n",
    "\n",
    "  Args:\n",
    "    input_features: The names of the numerical input features to use.\n",
    "  Returns:\n",
    "    A set of feature columns\n",
    "  \"\"\" \n",
    "  return set([tf.feature_column.numeric_column(my_feature)\n",
    "              for my_feature in input_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Return next batch (Of type of Tensorflow Dataset)\n",
    "def input_fn(features, targets, batch_size=1, shuffle=True, num_epochs=None):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((dict(features), targets))\n",
    "    \n",
    "    if(shuffle):\n",
    "        dataset = dataset.shuffle(1000)\n",
    "    \n",
    "    dataset = dataset.batch(batch_size).repeat(num_epochs)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_linear_classifier_model(\n",
    "    learning_rate,\n",
    "    steps,\n",
    "    batch_size,\n",
    "    training_examples,\n",
    "    training_targets,\n",
    "    validation_examples,\n",
    "    validation_targets):\n",
    "    \n",
    "    periods = 10\n",
    "    steps_per_period = steps / periods\n",
    "    \n",
    "    #Initialize Linear Classifier\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "    optimizer = tf.contrib.estimator.clip_gradients_by_norm(optimizer, 5.0)\n",
    "    linear_classifier = tf.estimator.LinearClassifier(\n",
    "        feature_columns=construct_feature_columns(training_examples),\n",
    "        optimizer=optimizer\n",
    "    )\n",
    "    \n",
    "    #Training Functions\n",
    "    training_input_fn = lambda: input_fn(\n",
    "        training_examples,\n",
    "        training_targets,\n",
    "        batch_size=batch_size) \n",
    "    predict_training_input_fn = lambda: input_fn(\n",
    "        training_examples, \n",
    "        training_targets, \n",
    "        num_epochs=1, \n",
    "        shuffle=False)\n",
    "    \n",
    "    #Validation Function\n",
    "    predict_validation_input_fn = lambda: input_fn(\n",
    "        validation_examples, \n",
    "        validation_targets, \n",
    "        num_epochs=1, \n",
    "        shuffle=False)\n",
    "    \n",
    "    #Train Model\n",
    "    training_log_losses = []\n",
    "    validation_log_losses = []\n",
    "    \n",
    "    print(\"Training Model\")\n",
    "    for period in range(0, periods):\n",
    "        linear_classifier.train(\n",
    "            input_fn=training_input_fn,\n",
    "            #Manually break total steps by 10\n",
    "            steps=steps_per_period\n",
    "        )\n",
    "        \n",
    "        #Use Sklearn to calculate Log Loss\n",
    "        training_probabilities = linear_classifier.predict(input_fn=predict_training_input_fn)\n",
    "        training_probabilities = np.array([item['probabilities'][0] for item in training_probabilities])\n",
    "        training_log_loss = metrics.log_loss(training_targets, training_probabilities)\n",
    "        \n",
    "        #Calculate Validation Log Loss\n",
    "        validation_probabilities = linear_classifier.predict(input_fn=predict_validation_input_fn)\n",
    "        validation_probabilities = np.array([item['probabilities'][0] for item in validation_probabilities])\n",
    "        validation_log_loss = metrics.log_loss(validation_targets, validation_probabilities)\n",
    "        \n",
    "        #Append Losses\n",
    "        training_log_losses.append(training_log_loss)\n",
    "        validation_log_losses.append(validation_log_loss)\n",
    "        \n",
    "        print(\"Period:\", period, \"Log Loss:\", training_log_loss)\n",
    "    print(\"Training Finished\")\n",
    "    \n",
    "    #Graph\n",
    "    plt.ylabel(\"Log Loss\")\n",
    "    plt.xlabel(\"Periods\")\n",
    "    plt.title(\"Log Loss vs. Periods\")\n",
    "    plt.tight_layout()\n",
    "    plt.plot(training_log_losses, label=\"training\")\n",
    "    plt.plot(validation_log_losses, label=\"validation\")\n",
    "    plt.legend()\n",
    "    \n",
    "    return linear_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dnn_classifier_model(\n",
    "    learning_rate,\n",
    "    steps,\n",
    "    batch_size,\n",
    "    hidden_units,\n",
    "    training_examples,\n",
    "    training_targets,\n",
    "    validation_examples,\n",
    "    validation_targets):\n",
    "\n",
    "    periods = 10\n",
    "    steps_per_period = steps / periods\n",
    "    \n",
    "    #Initialize DNN Regressor\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "    optimizer = tf.contrib.estimator.clip_gradients_by_norm(optimizer, 5.0)\n",
    "    dnn_classifier = tf.estimator.DNNClassifier(\n",
    "        feature_columns=construct_feature_columns(training_examples),\n",
    "        n_classes=2,\n",
    "        hidden_units=hidden_units,\n",
    "        optimizer=optimizer,\n",
    "    )\n",
    "    \n",
    "    #Training Functions\n",
    "    training_input_fn = lambda: input_fn(\n",
    "        training_examples,\n",
    "        training_targets,\n",
    "        batch_size=batch_size) \n",
    "    predict_training_input_fn = lambda: input_fn(\n",
    "        training_examples, \n",
    "        training_targets, \n",
    "        num_epochs=1, \n",
    "        shuffle=False)\n",
    "    \n",
    "    #Validation Function\n",
    "    predict_validation_input_fn = lambda: input_fn(\n",
    "        validation_examples, \n",
    "        validation_targets, \n",
    "        num_epochs=1, \n",
    "        shuffle=False)\n",
    "    \n",
    "    #Train Model\n",
    "    training_log_losses = []\n",
    "    validation_log_losses = []\n",
    "    \n",
    "    print(\"Training Model\")\n",
    "    for period in range(0, periods):\n",
    "        dnn_classifier.train(\n",
    "            input_fn=training_input_fn,\n",
    "            #Manually break total steps by 10\n",
    "            steps=steps_per_period\n",
    "        )\n",
    "        \n",
    "        #Use Sklearn to calculate Log Loss\n",
    "        training_probabilities = dnn_classifier.predict(input_fn=predict_training_input_fn)\n",
    "        training_probabilities = np.array([item['probabilities'][0] for item in training_probabilities])\n",
    "        training_log_loss = metrics.log_loss(training_targets, training_probabilities)\n",
    "        \n",
    "        #Calculate Validation Log Loss\n",
    "        validation_probabilities = dnn_classifier.predict(input_fn=predict_validation_input_fn)\n",
    "        validation_probabilities = np.array([item['probabilities'][0] for item in validation_probabilities])\n",
    "        validation_log_loss = metrics.log_loss(validation_targets, validation_probabilities)\n",
    "        \n",
    "        #Append Losses\n",
    "        training_log_losses.append(training_log_loss)\n",
    "        validation_log_losses.append(validation_log_loss)\n",
    "        \n",
    "        print(\"Period:\", period, \"Log Loss:\", training_log_loss)\n",
    "    print(\"Training Finished\")\n",
    "    \n",
    "    #Graph\n",
    "    plt.ylabel(\"Log Loss\")\n",
    "    plt.xlabel(\"Periods\")\n",
    "    plt.title(\"Log Loss vs. Periods\")\n",
    "    plt.tight_layout()\n",
    "    plt.plot(training_log_losses, label=\"training\")\n",
    "    plt.plot(validation_log_losses, label=\"validation\")\n",
    "    plt.legend()\n",
    "    \n",
    "    return dnn_classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "linear_classifier = train_linear_classifier_model(\n",
    "    learning_rate=0.0000005,\n",
    "    steps=750,\n",
    "    batch_size=20,\n",
    "    training_examples=training_examples,\n",
    "    training_targets=training_targets,\n",
    "    validation_examples=validation_examples,\n",
    "    validation_targets=validation_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dnn_classifier = train_dnn_classifier_model(\n",
    "    learning_rate=0.0000005,\n",
    "    steps=750,\n",
    "    batch_size=20,\n",
    "    hidden_units=[3, 2],\n",
    "    training_examples=training_examples,\n",
    "    training_targets=training_targets,\n",
    "    validation_examples=validation_examples,\n",
    "    validation_targets=validation_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_examples, test_targets):\n",
    "    predict_test_input_fn = lambda: input_fn(\n",
    "        test_examples, \n",
    "        test_targets, \n",
    "        num_epochs=1, \n",
    "        shuffle=False)\n",
    "    \n",
    "    test_predictions = model.predict(input_fn=predict_test_input_fn)\n",
    "    test_predictions = np.array([item['probabilities'][0] for item in test_predictions])\n",
    "\n",
    "    for prediction, label in zip(test_predictions, test_targets): \n",
    "        if(prediction > 0.5 and label or prediction < 0.5 and not label):\n",
    "            print(\"Probabilities:\", prediction, \"Label:\", label)\n",
    "        else:\n",
    "            print(\"Probabilities:\", prediction, \"Label:\", label, \"WRONG\")\n",
    "    \n",
    "    return test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_probabilities_linear_classifier = test_model(linear_classifier, test_examples, test_targets)\n",
    "test_log_loss_linear_classifier = metrics.log_loss(test_targets, test_probabilities_linear_classifier)\n",
    "print(\"Test Log Loss:\", test_log_loss_linear_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_probabilities_dnn_classifier = test_model(dnn_classifier, test_examples, test_targets)\n",
    "test_log_loss_dnn_classifier = metrics.log_loss(test_targets, test_probabilities_dnn_classifier)\n",
    "print(\"Test Log Loss:\", test_log_loss_dnn_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Additional Info**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linear Classifier Weights\n",
    "for v in linear_classifier.get_variable_names():\n",
    "    print(v, linear_classifier.get_variable_value(v))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
